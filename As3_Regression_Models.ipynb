{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient: 2.048289137509908\n",
      "Intercept: -0.16115435525701827\n",
      "Predicted values for new x values: [[0.45333239]\n",
      " [0.86299021]\n",
      " [1.47747695]]\n"
     ]
    }
   ],
   "source": [
    "#Veronika Wendler\n",
    "#ID:20375377\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "#Download Regression_Dset.csv and use Feature1 in the dataset as the independent/predictor variable x,\n",
    "# and let Feature4 be the dependent/target variable y.\n",
    "\n",
    "#(a) Run simple linear regression to predict y from x. \n",
    "# Report the linear model you found. Predict the value of y for new x values 0.3, 0.5, and 0.8.\n",
    "\n",
    "headers = [\"Feature1\",\"Feature2\",\"Feature3\",\"Feature4\"]\n",
    "df = pd.read_csv('Regression_Dset.csv', delimiter=',', skiprows=[0], names=headers)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df = pd.read_csv('Regression_Dset.csv')\n",
    "\n",
    "x = df['Feature1'].values.reshape(-1, 1)\n",
    "y = df['Feature4'].values.reshape(-1, 1)\n",
    "\n",
    "#linear regression model\n",
    "reg = LinearRegression().fit(x, y)\n",
    "\n",
    "print('Coefficient:', reg.coef_[0][0])\n",
    "print('Intercept:', reg.intercept_[0])\n",
    "\n",
    "# Predict new x values\n",
    "new_x = [[0.3], [0.5], [0.8]]\n",
    "new_y = reg.predict(new_x)\n",
    "print('Predicted values for new x values:', new_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear model: y = 2.0483x + -0.1612\n",
      "Predicted values of y for new x values:\n",
      "x = 0.3: y = 0.4533\n",
      "x = 0.5: y = 0.8630\n",
      "x = 0.8: y = 1.4775\n"
     ]
    }
   ],
   "source": [
    "# (a) columns for independent and dependent variables\n",
    "X = df['Feature1'].values.reshape(-1, 1)\n",
    "y = df['Feature4'].values.reshape(-1, 1)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "print('Linear model: y = {:.4f}x + {:.4f}'.format(model.coef_[0][0], model.intercept_[0]))\n",
    "\n",
    "# new x values\n",
    "x_new = [[0.3], [0.5], [0.8]]\n",
    "y_new = model.predict(x_new)\n",
    "\n",
    "# predicted values\n",
    "print('Predicted values of y for new x values:')\n",
    "for i in range(len(x_new)):\n",
    "    print('x = {:.1f}: y = {:.4f}'.format(x_new[i][0], y_new[i][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MSE: 0.0211\n"
     ]
    }
   ],
   "source": [
    "#b  cross-validation to predict generalization error.\n",
    "\n",
    "# Initialize the k-fold cross-validator\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "mse_list = []\n",
    "\n",
    "# Loop over folds, training and test set\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    #training set Lin. Reg\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # mean squared error on the test set\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mse_list.append(mse)\n",
    "\n",
    "#average mean squared error over all folds\n",
    "avg_mse = sum(mse_list) / len(mse_list)\n",
    "\n",
    "print(\"Average MSE: {:.4f}\".format(avg_mse))\n",
    "\n",
    "\n",
    "#The code then loops over the folds, splitting the data into training and test sets,\n",
    "#  fitting a linear regression model on the training set, and predicting the target variable on the test set.\n",
    "#  It then calculates mean squared error between the predicted and actual target values and stores it in a list. \n",
    "# The code computes average mean squared error over all folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MSE: 0.0211\n",
      "\n",
      "Polynomial model for p = 2: Average MSE = 0.0060\n",
      "Model: y = 0.19 + 1.69x^1\n",
      "Predictions for x = 0.3, 0.5, and 0.8: [[0.41171329]\n",
      " [0.72665873]\n",
      " [1.45230822]]\n",
      "\n",
      "Polynomial model for p = 3: Average MSE = 0.0051\n",
      "Model: y = 0.09 + -0.56x^1 + 1.41x^2\n",
      "Predictions for x = 0.3, 0.5, and 0.8: [[0.44153266]\n",
      " [0.73172726]\n",
      " [1.42114407]]\n",
      "\n",
      "Polynomial model for p = 4: Average MSE = 0.0051\n",
      "Model: y = 0.09 + -0.62x^1 + 1.50x^2 + -0.04x^3\n",
      "Predictions for x = 0.3, 0.5, and 0.8: [[0.44165186]\n",
      " [0.73150481]\n",
      " [1.42139595]]\n",
      "\n",
      "Polynomial model for p = 5: Average MSE = 0.0052\n",
      "Model: y = 0.08 + -2.19x^1 + 5.39x^2 + -4.20x^3 + 1.60x^4\n",
      "Predictions for x = 0.3, 0.5, and 0.8: [[0.44079441]\n",
      " [0.73099614]\n",
      " [1.42078295]]\n"
     ]
    }
   ],
   "source": [
    "#c polynomial regression for p = 2; 3; 4; and 5.\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Initialize k-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "mse_list = []\n",
    "\n",
    "# Loop and tranings and test sets\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mse_list.append(mse)\n",
    "\n",
    "# average mse over all folds\n",
    "avg_mse = sum(mse_list) / len(mse_list)\n",
    "\n",
    "print(\"Average MSE: {:.4f}\".format(avg_mse))\n",
    "\n",
    "#polynomial regression models of order p = 2 to 5\n",
    "for p in range(2, 6):\n",
    "    X_poly = np.concatenate([X ** i for i in range(1, p + 1)], axis=1)\n",
    "    \n",
    "    # training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    poly_reg = LinearRegression()\n",
    "    poly_reg.fit(X_train, y_train)\n",
    "    y_pred = poly_reg.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    print(\"\\nPolynomial model for p = {}: Average MSE = {:.4f}\".format(p, mse))\n",
    "    \n",
    "    #polynomial model\n",
    "    coeffs = poly_reg.coef_[0]\n",
    "    intercept = poly_reg.intercept_\n",
    "    model_str = \"{:.2f}\".format(intercept[0])\n",
    "    for i in range(1, len(coeffs)):\n",
    "        model_str += \" + {:.2f}x^{}\".format(coeffs[i], i)\n",
    "    print(\"Model: y = {}\".format(model_str))\n",
    "    \n",
    "    #predictions for x = 0.3, 0.5, and 0.8\n",
    "    x_values = np.array([0.3, 0.5, 0.8]).reshape(-1, 1)\n",
    "    x_poly = np.concatenate([x_values ** i for i in range(1, p + 1)], axis=1)\n",
    "    y_pred = poly_reg.predict(x_poly)\n",
    "    \n",
    "    print(\"Predictions for x = 0.3, 0.5, and 0.8: {}\".format(y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best polynomial order: p = 3, Average MSE = 0.0054\n"
     ]
    }
   ],
   "source": [
    "#d \n",
    "#To cross-validate and choose the best model,I added a nested loop perfomring k-fold\n",
    "#  cross-validation for each polynomial order from 2 to 5.\n",
    "# The code selects the polynomialthe lowest average mean squared error as the best model\n",
    "# after computing the average mean squared error over all folds for each polynomial order, the \n",
    "#The code uses KFold from sklearn.model_selection to perform k-fold cross-validation, with n_splits=5.The outer loop iterates over the\n",
    "# polynomial orders from 2 to 5 , inner loop performs k-fold cross-validation. \n",
    "# data splits into training and test sets using indices provided by kf_outer.split. The linear regression model is trained on the training set using LinearRegression().fit, and\n",
    "#  then used to predict the target variable on the test set using model.predict. \n",
    "\n",
    "df = pd.read_csv('Regression_Dset.csv')\n",
    "\n",
    "X = df['Feature1'].values.reshape(-1, 1)\n",
    "y = df['Feature4'].values.reshape(-1, 1)\n",
    "kf_outer = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "avg_mse_list = []\n",
    "\n",
    "# Loop over the polynomials\n",
    "for p in range(2, 6):\n",
    "    \n",
    "    mse_list = []\n",
    "    X_poly = np.concatenate([X ** i for i in range(1, p + 1)], axis=1)\n",
    "    \n",
    "    for train_idx, test_idx in kf_outer.split(X_poly):\n",
    "        X_train, X_test = X_poly[train_idx], X_poly[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "        mse_list.append(mse)\n",
    "    \n",
    "    avg_mse = sum(mse_list) / len(mse_list)\n",
    "    \n",
    "    avg_mse_list.append(avg_mse)\n",
    "\n",
    "#polynomial order with lowest average mean squared error\n",
    "best_p = np.argmin(avg_mse_list) + 2\n",
    "\n",
    "# best polynomial order and its average mean squared error\n",
    "print(\"Best polynomial order: p = {}, Average MSE = {:.4f}\".format(best_p, avg_mse_list[best_p - 2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values for new data:\n",
      "(0.3, 0.4, 0.1) => 0.4396\n",
      "(0.5, 0.2, 0.4) => 0.9749\n",
      "(0.8, 0.2, 0.7) => 1.6804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Neuroeconomics Lab\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#2a\n",
    "df = pd.read_csv('Regression_Dset.csv')\n",
    "\n",
    "X = df[['Feature1', 'Feature2', 'Feature3']]\n",
    "y = df['Feature4']\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "new_values = np.array([[0.3, 0.4, 0.1], [0.5, 0.2, 0.4], [0.8, 0.2, 0.7]])\n",
    "y_pred = model.predict(new_values)\n",
    "print(\"Predicted values for new data:\")\n",
    "for i, val in enumerate(y_pred):\n",
    "    print(\"({:.1f}, {:.1f}, {:.1f}) => {:.4f}\".format(new_values[i][0], new_values[i][1], new_values[i][2], val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'polynomialfeatures__degree': 3, 'ridge__alpha': 0.01}\n",
      "Best mean squared error: 0.00538516162439244\n",
      "Predicted target variable for x_new:\n",
      " [0.45519428 0.69133496 1.43433698]\n"
     ]
    }
   ],
   "source": [
    "#b \n",
    "#To regularize the model, we can use Ridge or Lasso regression. In Ridge regression, we add a penalty term to the cost function \n",
    "#that shrinks the coefficient values towards zero \n",
    "#Here is an example of how to use Ridge regression with polynomial features and cross-validation to choose the best hyperparameter value:\n",
    "#In this example, we create a pipeline that first applies polynomial features to the data and then applies Ridge regression.\n",
    "# we define a hyperparameter grid with different values for the polynomial degree and the Ridge regression alpha parameter, \n",
    "# and use cross-validation to choose the best hyperparameters based on the mean squared error. We then use the best model to predict the target\n",
    "# variable for new (x1,x2,x3) values.\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "df = pd.read_csv('Regression_Dset.csv')\n",
    "\n",
    "X = df[['Feature1', 'Feature2', 'Feature3']].values\n",
    "y = df['Feature4'].values\n",
    "\n",
    "#pipeline with polynomial features and Ridge regression\n",
    "model = make_pipeline(PolynomialFeatures(), Ridge())\n",
    "\n",
    "# param grid for cross-validation\n",
    "param_grid = {'polynomialfeatures__degree': [2, 3, 4, 5],\n",
    "              'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# cross-validation to choose the best hyperparameter values\n",
    "grid_search = GridSearchCV(model, param_grid, cv=kf, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# best hyperparameter values and the corresponding mean squared error\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best mean squared error:\", -grid_search.best_score_)\n",
    "\n",
    "# best model to predict the target variable for new (x1,x2,x3) values\n",
    "x_new = np.array([[0.3, 0.4, 0.1], [0.5, 0.2, 0.4], [0.8, 0.2, 0.7]])\n",
    "y_new = grid_search.predict(x_new)\n",
    "\n",
    "print(\"Predicted target variable for x_new:\\n\", y_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of linear regression model: 0.016450262907705834\n",
      "MSE of polynomial regression model: 0.0051023187610121645\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#Based on the mean squared error (MSE) values, we can see that the polynomial regression model with Ridge\n",
    "#  regularization has a smaller MSE (0.0051) compared to the linear regression model (0.0165). This suggests\n",
    "#  that the polynomial regression model generalizes better to new data and has a lower chance of overfitting. \n",
    "# The polynomial regression model is likely to be a better choice for predicting the target variable for new data points.\n",
    "\n",
    "df = pd.read_csv('Regression_Dset.csv')\n",
    "\n",
    "X = df[['Feature1', 'Feature2', 'Feature3']].values\n",
    "y = df['Feature4'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model_a = LinearRegression()\n",
    "model_a.fit(X_train, y_train)\n",
    "y_pred_a = model_a.predict(X_test)\n",
    "mse_a = mean_squared_error(y_test, y_pred_a)\n",
    "\n",
    "# pipeline with polynomial features and ridge regression\n",
    "model_b = make_pipeline(PolynomialFeatures(), Ridge(alpha=grid_search.best_params_['ridge__alpha'],\n",
    "                                                    max_iter=10000))\n",
    "model_b.steps[0][1].set_params(degree=grid_search.best_params_['polynomialfeatures__degree'])\n",
    "model_b.fit(X_train, y_train)\n",
    "\n",
    "y_pred_b = model_b.predict(X_test)\n",
    "mse_b = mean_squared_error(y_test, y_pred_b)\n",
    "\n",
    "# mean squared error of both models\n",
    "print(\"MSE of linear regression model: {}\".format(mse_a))\n",
    "print(\"MSE of polynomial regression model: {}\".format(mse_b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "bunchData = load_breast_cancer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n"
     ]
    }
   ],
   "source": [
    "print(bunchData.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the dependent/target feature\n",
    "y_data=pd.DataFrame(bunchData[\"target\"],columns=[\"cellType\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the class labels\n",
    "class_labels=bunchData[\"target_names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the independent/predictor features \n",
    "x_data=pd.DataFrame(bunchData[\"data\"],columns=bunchData[\"feature_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for k=4: 0.9210526315789473\n",
      "Accuracy score for k=6: 0.9385964912280702\n",
      "Accuracy score for k=10: 0.9473684210526315\n",
      "Accuracy score for k=50: 0.9473684210526315\n",
      "KNN classifier with k=4:\n",
      "Jaccard index: 0.8714285714285714\n",
      "F1-score: 0.931297709923664\n",
      "Log loss: 1.0060801143887097\n",
      "\n",
      "KNN classifier with k=6:\n",
      "Jaccard index: 0.9\n",
      "F1-score: 0.9473684210526316\n",
      "Log loss: 0.9901604188378291\n",
      "\n",
      "KNN classifier with k=10:\n",
      "Jaccard index: 0.9142857142857143\n",
      "F1-score: 0.9552238805970149\n",
      "Log loss: 0.7008216656826886\n",
      "\n",
      "KNN classifier with k=50:\n",
      "Jaccard index: 0.9166666666666666\n",
      "F1-score: 0.9565217391304348\n",
      "Log loss: 0.16995724239700638\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Neuroeconomics Lab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\Neuroeconomics Lab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\Neuroeconomics Lab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\Neuroeconomics Lab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\Neuroeconomics Lab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "c:\\Users\\Neuroeconomics Lab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\Neuroeconomics Lab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "c:\\Users\\Neuroeconomics Lab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\Neuroeconomics Lab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "c:\\Users\\Neuroeconomics Lab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\Neuroeconomics Lab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "c:\\Users\\Neuroeconomics Lab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "#Analyze the extracted data and train various classifiers using the following algorithms: \n",
    "# a) KNN for k=4, k=6, k=10, and k=50;\n",
    "\n",
    "#best model is the SVM classifier with a linear kernel. It has the highest Jaccard index and F1-score, and the lowest log loss.\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=0)\n",
    "\n",
    "#KNN classifiersdifferent values of k\n",
    "k_values = [4, 6, 10, 50]\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train, y_train.values.ravel())\n",
    "    y_pred = knn.predict(x_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy score for k={k}: {acc}\")\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import jaccard_score, f1_score, log_loss\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=0)\n",
    "\n",
    "#classifiers with different values of k\n",
    "k_values = [4, 6, 10, 50]\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train, y_train)\n",
    "    y_pred = knn.predict(x_test)\n",
    "\n",
    "    #Jaccard index, F1-score, and Log loss\n",
    "    jac = jaccard_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    logloss = log_loss(y_test, knn.predict_proba(x_test))\n",
    "\n",
    "    print(f\"KNN classifier with k={k}:\")\n",
    "    print(f\"Jaccard index: {jac}\")\n",
    "    print(f\"F1-score: {f1}\")\n",
    "    print(f\"Log loss: {logloss}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for rbf kernel: 0.9298245614035088\n",
      "Accuracy score for linear kernel: 0.956140350877193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Neuroeconomics Lab\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM with linear kernel:\n",
      "Jaccard index: 0.9264705882352942\n",
      "F1-score: 0.9618320610687023\n",
      "Log loss: 0.11345483783136227\n",
      "\n",
      "SVM with rbf kernel:\n",
      "Jaccard index: 0.8918918918918919\n",
      "F1-score: 0.9428571428571428\n",
      "Log loss: 0.1648996454068956\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Neuroeconomics Lab\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "#In terms of Jaccard index and F1-score, the Decision Tree models with a depth of 2 or 3 perform best. \n",
    "# However, if we look at the accuracy score, the SVM model with a linear kernel has the highest accuracy (0.9561) \n",
    "# followed closely by the Decision Tree with a depth of 3 (0.9649). Moreover, the log loss is the lowest for the SVM with a \n",
    "# linear kernel (0.113), which indicates better calibration of the model's predicted probabilities.\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=0)\n",
    "\n",
    "#SVM  'rbf' and 'linear' kernels\n",
    "kernel_functions = ['rbf', 'linear']\n",
    "for kernel in kernel_functions:\n",
    "    svm = SVC(kernel=kernel)\n",
    "    svm.fit(x_train, y_train.values.ravel())\n",
    "    y_pred = svm.predict(x_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy score for {kernel} kernel: {acc}\")\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import jaccard_score, f1_score, log_loss\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=0)\n",
    "\n",
    "# SVM classifier\n",
    "kernels = ['linear','rbf']\n",
    "for kernel in kernels:\n",
    "    svm = SVC(kernel=kernel, random_state=0, probability=True)\n",
    "    svm.fit(x_train, y_train)\n",
    "    y_pred = svm.predict(x_test)\n",
    "\n",
    "    # Jaccard index, F1-score, and Log loss\n",
    "    jac = jaccard_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    logloss = log_loss(y_test, svm.predict_proba(x_test))\n",
    "\n",
    "    print(f\"SVM with {kernel} kernel:\")\n",
    "    print(f\"Jaccard index: {jac}\")\n",
    "    print(f\"F1-score: {f1}\")\n",
    "    print(f\"Log loss: {logloss}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for max_depth=2: 0.9649122807017544\n",
      "Accuracy score for max_depth=3: 0.9649122807017544\n",
      "Accuracy score for max_depth=4: 0.956140350877193\n",
      "Accuracy score for max_depth=10: 0.9122807017543859\n",
      "Decision tree with depth 2:\n",
      "Jaccard index: 0.9428571428571428\n",
      "F1-score: 0.9705882352941176\n",
      "Log loss: 1.211907933087135\n",
      "\n",
      "Decision tree with depth 3:\n",
      "Jaccard index: 0.9428571428571428\n",
      "F1-score: 0.9705882352941176\n",
      "Log loss: 1.211907933087135\n",
      "\n",
      "Decision tree with depth 4:\n",
      "Jaccard index: 0.9285714285714286\n",
      "F1-score: 0.962962962962963\n",
      "Log loss: 1.5148796558495095\n",
      "\n",
      "Decision tree with depth 10:\n",
      "Jaccard index: 0.8571428571428571\n",
      "F1-score: 0.923076923076923\n",
      "Log loss: 3.0297382696613813\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#decision tree\n",
    "\n",
    "#the best model is the Decision Tree with a depth of 2 or 3. \n",
    "# Both models have the same accuracy score of 0.9649122807017544, \n",
    "# which is the highest among all the models tested. Additionally,\n",
    "#  they have high Jaccard index and F1-score and relatively low log loss,\n",
    "#  indicating good performance in both precision and recall, as well as low uncertainty in classification. \n",
    "# The decision tree with depth 4 also performed well, but slightly lower than the previous two models,\n",
    "#  while the decision tree with depth 10 has the lowest performance among all models tested. \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#decision tree depths\n",
    "depths = [2, 3, 4, 10]\n",
    "for depth in depths:\n",
    "    dtc = DecisionTreeClassifier(max_depth=depth)\n",
    "    dtc.fit(x_train, y_train.values.ravel())\n",
    "    y_pred = dtc.predict(x_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy score for max_depth={depth}: {acc}\")\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import jaccard_score, f1_score, log_loss\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=0)\n",
    "\n",
    "# Decision tree classifiers\n",
    "depths = [2, 3, 4, 10]\n",
    "for depth in depths:\n",
    "    dt = DecisionTreeClassifier(max_depth=depth, random_state=0)\n",
    "    dt.fit(x_train, y_train)\n",
    "    y_pred = dt.predict(x_test)\n",
    "\n",
    "    jac = jaccard_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    logloss = log_loss(y_test, y_pred)\n",
    "\n",
    "    print(f\"Decision tree with depth {depth}:\")\n",
    "    print(f\"Jaccard index: {jac}\")\n",
    "    print(f\"F1-score: {f1}\")\n",
    "    print(f\"Log loss: {logloss}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard index: 0.9130434782608695\n",
      "F1-score: 0.9545454545454547\n",
      "Log loss: 1.8178443645993383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Neuroeconomics Lab\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#logistic \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import jaccard_score, f1_score, log_loss\n",
    "\n",
    "# logistic regression classifier\n",
    "lr = LogisticRegression(random_state=0)\n",
    "lr.fit(x_train, y_train.values.ravel())\n",
    "y_pred = lr.predict(x_test)\n",
    "\n",
    "jac = jaccard_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "logloss = log_loss(y_test, y_pred)\n",
    "\n",
    "print(f\"Jaccard index: {jac}\")\n",
    "print(f\"F1-score: {f1}\")\n",
    "print(f\"Log loss: {logloss}\")\n",
    "\n",
    "\n",
    "#evaluate the model using the Jaccard index, F1-score, and Log loss metrics, which are appropriate\n",
    "#  for evaluating binary classification models.\n",
    "# Log loss measures predicted probabilities.The Jaccard index and F1-score are both measures of accuracy that take into\n",
    "# account both true positives and false positives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the best model among all of the models would be the SVM with linear kernel, \n",
    "# which has a Jaccard index of 0.926, an F1-score of 0.962, and a log loss of 0.113. \n",
    "# It has the highest accuracy score of 0.956, which is also an indication of its good performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Vero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
